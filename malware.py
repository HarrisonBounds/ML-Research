import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import GridSearchCV
from sklearn.feature_selection import SelectFromModel
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
import matplotlib.pyplot as plt
import seaborn as sns
import time
from sdv.single_table import CTGANSynthesizer
from sdv.single_table import GaussianCopulaSynthesizer
from sdv.metadata import SingleTableMetadata
from sdv.evaluation.single_table import run_diagnostic
from sdv.evaluation.single_table import evaluate_quality
from CTABGAN import gan
from table_evaluator import TableEvaluator
import json


def encode_labels(y_train, y_test):
    #Covert the text labels to numbers so the model can understand it
    label_encoder = LabelEncoder()

    y_train_encoded = label_encoder.fit_transform(y_train)
    y_test_encoded = label_encoder.fit_transform(y_test)

    class_names = label_encoder.classes_

    print("Class names: ", class_names)

    return y_train_encoded, y_test_encoded, class_names



def binary_classification(X_train, y_train, X_test, y_test, num_trees):
    positive_class = "BenignTraffic"

    #Create binary labels where benign is 1 and malicious is 0
    y_train_binary = (y_train == positive_class).astype(int)
    y_test_binary = (y_test == positive_class).astype(int)

    #Training binary classifier
    binary_classifier = RandomForestClassifier(n_estimators=num_trees, criterion='entropy', class_weight='balanced', random_state=50)
    binary_classifier.fit(X_train, y_train_binary)

    # Predict using the binary classifier
    y_pred_binary = binary_classifier.predict(X_test)
    y_pred_prob_binary = binary_classifier.predict_proba(X_test)[:, 1]  # Use the probability for the positive class

    return y_test_binary, y_pred_binary, y_pred_prob_binary


def multi_class_classification(X_train, y_train_encoded, X_test, num_trees, classification, feature_selection):
    print("Training...")
    start_time = time.time()
    
    #Feature Selection
    if feature_selection:
        sel = SelectFromModel(RandomForestClassifier(n_estimators=num_trees, criterion='gini', min_samples_leaf=2, max_depth=None, verbose=True))
        sel.fit(X_train, y_train_encoded)

        X_train = sel.transform(X_train)
        X_test = sel.transform(X_test)
    
    else:
        X_train = X_train
        X_test = X_test
    
    #Classification selection
    if classification == "RF":
        classifier = RandomForestClassifier(n_estimators=num_trees, criterion='gini', min_samples_leaf=2, max_depth=None, verbose=True)
    elif classification == "MLP":
        mlp = MLPClassifier(max_iter=300)

        #Finding the best parameters
        parameter_space = {
            'hidden_layer_sizes': [(100, 100, 100), (256, 256, 256), (100, 164, 256), (256, 100, 50, 50)],
            'activation': ['tanh', 'relu'],
            'solver': ['sgd', 'adam'],
            'alpha': [0.0001, 0.001, 0.01, 0.05, 0.005],
            'learning_rate': ['constant', 'adaptive']
        }

        classifier = GridSearchCV(mlp, parameter_space, n_jobs=-1, cv=3)
    elif classification == "NB":
        classifier = GaussianNB()

    classifier.fit(X_train, y_train_encoded)
    end_time = time.time()

    print(f'It took {(end_time-start_time)/60} minutes to train')

    print("Predicting...")
    start_time = time.time()
    y_pred = classifier.predict(X_test)
    end_time = time.time()

    print(f'It took {(end_time-start_time)/60} minutes to predict')
    
    return y_pred
    
def show_confusion_matrix(y_test, y_pred, class_names, classification):
    confusion_mat = confusion_matrix(y_test, y_pred)

    plt.figure(figsize=(14, 10))
    sns.heatmap(confusion_mat, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.title(f'Malware Confusion Matrix using {classification} classification')
    plt.show()

def main():
    #Load config file
    with open("config.json", "r") as f:
        data = json.load(f)

    df = pd.read_csv(data["mal_path"])

    #Clip the random string of letters and numbers in the label column
    df['Category'] = df['Category'].str.split('-').str.slice(stop=2).str.join('-')

    #Only classify malicious data
    if data["remove_benign"]:
        df = df[df[data["label_column"]] != 'Benign']

    if data["mal_type"]:
        df = df[df[data["label_column"]].str.contains(data["mal_type"])]

    #Drop the 'Malware' column
    df = df.drop(data["class_column"], axis=1)

    if data["gan_model"]: 
         # Filter dataframe to include only the desired class
        df_ra = df[df['Category'] == 'Ransomware-Ako']
        df_rc = df[df['Category'] == 'Ransomware-Conti']
        df_rm = df[df['Category'] == 'Ransomware-Maze']
        df_rp = df[df['Category'] == 'Ransomware-Pysa']
        df_rs = df[df['Category'] == 'Ransomware-Shade']

        df_s1 = df[df['Category'] == 'Spyware-180solutions']
        df_sc = df[df['Category'] == 'Spyware-CWS']
        df_sg = df[df['Category'] == 'Spyware-Gator']
        df_stibs = df[df['Category'] == 'Spyware-TIBS']
        df_str = df[df['Category'] == 'Spyware-Transponder']

        df_te = df[df['Category'] == 'Trojan-Emotet']
        df_trec = df[df['Category'] == 'Trojan-Reconyc']
        df_tref = df[df['Category'] == 'Trojan-Refroso']
        df_ts = df[df['Category'] == 'Trojan-Scar']
        df_tz = df[df['Category'] == 'Trojan-Zeus']


        #Convert dataframe into metadata
        metadata = SingleTableMetadata()
        metadata.detect_from_dataframe(df_s1)

        #Generate synthetic data
        print("Generating Synthetic Data...")
        #start_time = time.time()
        synthesizer = CTGANSynthesizer(metadata, verbose=True)
        pre_preocessed_data = synthesizer.preprocess(df_s1)
        synthesizer.fit(pre_preocessed_data)

        synthetic_data_s1 = synthesizer.sample(num_rows=data["num_generated_rows"])

        metadata = SingleTableMetadata()
        metadata.detect_from_dataframe(df_sc)

        #Generate synthetic data
        print("Generating Synthetic Data...")
        #start_time = time.time()
        synthesizer = CTGANSynthesizer(metadata, verbose=True)
        pre_preocessed_data = synthesizer.preprocess(df_sc)
        synthesizer.fit(pre_preocessed_data)

        synthetic_data_sc = synthesizer.sample(num_rows=data["num_generated_rows"])

        metadata = SingleTableMetadata()
        metadata.detect_from_dataframe(df_sg)

        #Generate synthetic data
        print("Generating Synthetic Data...")
        #start_time = time.time()
        synthesizer = CTGANSynthesizer(metadata, verbose=True)
        pre_preocessed_data = synthesizer.preprocess(df_sg)
        synthesizer.fit(pre_preocessed_data)

        synthetic_data_sg = synthesizer.sample(num_rows=data["num_generated_rows"])

        metadata = SingleTableMetadata()
        metadata.detect_from_dataframe(df_stibs)

        #Generate synthetic data
        print("Generating Synthetic Data...")
        #start_time = time.time()
        synthesizer = CTGANSynthesizer(metadata, verbose=True)
        pre_preocessed_data = synthesizer.preprocess(df_stibs)
        synthesizer.fit(pre_preocessed_data)

        synthetic_data_stibs = synthesizer.sample(num_rows=data["num_generated_rows"])

        metadata = SingleTableMetadata()
        metadata.detect_from_dataframe(df_str)

        #Generate synthetic data
        print("Generating Synthetic Data...")
        #start_time = time.time()
        synthesizer = CTGANSynthesizer(metadata, verbose=True)
        pre_preocessed_data = synthesizer.preprocess(df_str)
        synthesizer.fit(pre_preocessed_data)

        synthetic_data_str = synthesizer.sample(num_rows=data["num_generated_rows"])


        #end_time = time.time()

        #print(f'It took {(end_time-start_time)/60} minutes to generate {data["num_generated_rows"]} rows')

        if data["train_synthetic"]:
            df = synthetic_data_str

        else:
            # #How good is the generated data?
            # diagnostic_report = run_diagnostic(
            #     real_data=df,
            #     synthetic_data=synthetic_data,
            #     metadata=metadata,
            #     verbose=True)
            
            # quality_report = evaluate_quality(
            #     real_data=df,
            #     synthetic_data=synthetic_data,
            #     metadata=metadata,
            #     verbose=True)
        

            
            df = pd.concat([df, synthetic_data_s1], ignore_index=True)
            df = pd.concat([df, synthetic_data_sc], ignore_index=True)
            df = pd.concat([df, synthetic_data_sg], ignore_index=True)
            df = pd.concat([df, synthetic_data_stibs], ignore_index=True)
            df = pd.concat([df, synthetic_data_str], ignore_index=True)

    
    X = df.drop([data["label_column"]], axis=1)
    y = df[data["label_column"]]

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, shuffle=True)

    #Get the encoded labels
    y_train_encoded, y_test_encoded, class_names = encode_labels(y_train, y_test)

    #Model you want to run
    y_pred = multi_class_classification(X_train, y_train_encoded, X_test, data["num_trees"], data["classification"], data["feature_selection"])

    #Evaluate the model
    print(classification_report(y_test_encoded, y_pred, target_names=class_names))

    #Display confusion matrix
    show_confusion_matrix(y_test_encoded, y_pred, class_names, data["classification"])



       
if __name__ == "__main__":
    main()